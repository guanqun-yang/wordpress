---
title: Research Notes | Machine Learning
tags: 
categories:
- Research
---

# Overview

The following notes are organized by and taken from the books below:

-   Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2rd Edition; this book's 3rd edition has been released in 2022.

# Dimensionality Reduction

The notion of "curse of dimensionality" does not arise solely regarding computation: more features makes the computation slow. It is also backed up by some theoretical observations. Specifically, consider the **unit** square, cube, or hypercube of 2, 3, through 10000 dimensions, we consider (1) when we sample one point, the probability it is within 0.001 to the border is $1 - (1 - 0.001) ^ d$, (2) when we sample two points, the average distance of these two points is roughly $\sqrt{d/6}$ (see [answer](https://math.stackexchange.com/a/2985722/488804)).

This indicate that the in high dimensional space (1) any point is likely to be close to the border because it is easy for a point to be an extremist in one dimension with an increase in the number of dimensions, (2) the points are sparse; this sparsity could only be remedied by exponentially more samples with respect to dimension $d$, which is infeasible in practice.

